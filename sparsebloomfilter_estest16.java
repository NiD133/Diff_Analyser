package org.apache.commons.collections4.bloomfilter;

import org.junit.Test;

/**
 * This test class contains a specific test case for the {@link SparseBloomFilter},
 * focusing on its behavior under extreme memory conditions.
 *
 * <p>The original test was auto-generated by EvoSuite and has been refactored
 * to improve its clarity, expressiveness, and maintainability.</p>
 */
// The original test extended a scaffolding class, which is preserved for compatibility.
public class SparseBloomFilter_ESTestTest16 extends SparseBloomFilter_ESTest_scaffolding {

    /**
     * Tests that attempting to merge a hasher into a SparseBloomFilter configured
     * with an impractically large number of bits (Integer.MAX_VALUE) results in an
     * OutOfMemoryError.
     *
     * <p>This scenario simulates an extreme edge case. The {@code SparseBloomFilter}
     * internally uses a {@code TreeSet} to store the indices of enabled bits. When the
     * hasher generates indices across this vast range, the TreeSet attempts to allocate
     * memory for each unique index, which is expected to exhaust the heap.</p>
     *
     * <p><b>Note:</b> While tests that expect an {@code OutOfMemoryError} are generally
     * considered fragile, this test is valuable for documenting the behavior of the
     * implementation under extreme memory pressure and confirming its practical limits.</p>
     */
    @Test(timeout = 4000, expected = OutOfMemoryError.class)
    public void mergeShouldThrowOOMForExtremelyLargeShape() {
        // Arrange: Create a Shape with the maximum possible number of bits.
        // This configuration is not practical for real-world use but is designed
        // specifically to exhaust available memory.
        final int numberOfItems = 3;
        final int numberOfBits = Integer.MAX_VALUE;
        final Shape extremelyLargeShape = Shape.fromNM(numberOfItems, numberOfBits);
        final SparseBloomFilter bloomFilter = new SparseBloomFilter(extremelyLargeShape);

        // Create a standard hasher. The specific data used for hashing is not critical.
        final Hasher hasher = new EnhancedDoubleHasher(3, 3);

        // Act & Assert: Merging the hasher will generate indices up to numberOfBits.
        // The underlying TreeSet will attempt to store these, causing an
        // OutOfMemoryError due to the enormous number of potential indices.
        // The @Test(expected) annotation handles the assertion.
        bloomFilter.merge(hasher);
    }
}